{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ <b><u>Exercise objectives</u></b>\n",
    "- Understand the *MNIST* dataset \n",
    "- Design your first **Convolutional Neural Network** (*CNN*) and answer questions such as:\n",
    "    - what are *Convolutional Layers*? \n",
    "    - how many *parameters* are involved in such a layer?\n",
    "- Train this CNN on images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ <b><u>Let's get started!</u></b>\n",
    "\n",
    "Imagine that we are  back in time into the 90's.\n",
    "You work at a *Post Office* and you have to deal with an enormous amount of letters on a daily basis. How could you automate the process of reading the ZIP Codes, which are a combination of 5 handwritten digits? \n",
    "\n",
    "This task, called the **Handwriting Recognition**, used to be a very complex problem back in those days. It was solved by *Bell Labs* (among others) where one of the Deep Learning gurus, *Yann Le Cun* used to work.\n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Handwriting_recognition):\n",
    "\n",
    "> Handwriting recognition (HWR), also known as Handwritten Text Recognition (HTR), is the ability of a computer to receive and interpret intelligible handwritten input from sources such as paper documents, photographs, touch-screens and other devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Number recognition](recognition.gif)\n",
    "\n",
    "*Note: The animation above is just here to help you visualize what happens with the different images: <br/> $\\rightarrow$ For each image, the CNN predicts what digit is written. The inputs are the different digits and not one animation/video!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î <b><u>How does this CNN work ?</u></b>\n",
    "\n",
    "- *Inputs*: Images (_each image shows a handwritten digit_)\n",
    "- *Target*: For each image, you want your CNN model to predict the correct digit (between 0 and 9)\n",
    "    - It is a **multi-class classification** task (more precisely a 10-class classification task since there are 10 different digits).\n",
    "\n",
    "üî¢ To improve the capacity of the Convolutional Neural Network to read these numbers, we need to feed it with many images representing handwritten digits. This is why the üìö [**MNIST dataset**] üìö(http://yann.lecun.com/exdb/mnist/) *(Mixed National Institute of Standards and Technology)* was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) The `MNIST` Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö Tensorflow/Keras offers multiple [**datasets**](https://www.tensorflow.org/api_docs/python/tf/keras/datasets) to play with:\n",
    "- *Vectors*: `boston_housing` (regression)\n",
    "- *Images* : `mnist`, `fashion_mnist`, `cifar10`, `cifar100` (classification)\n",
    "- *Texts*: `imbd`, `reuters` (classification/sentiment analysis)\n",
    "\n",
    "\n",
    "üíæ You can **load the MNIST dataset** with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data(path=\"mnist.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "delete"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((60000, 28, 28), (60000,)), ((10000, 28, 28), (10000,)))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1) Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Let's have look at some handwritten digits of this MNIST dataset.** ‚ùì\n",
    "\n",
    "üñ® Print some images from the *train set*.\n",
    "\n",
    "<details>\n",
    "    <summary><i>Hints</i></summary>\n",
    "\n",
    "üí°*Hint*: use the `imshow` function from `matplotlib` with `cmap = \"gray\"`\n",
    "\n",
    "ü§® Note: if you don't specify this *cmap* argument, the weirdly displayed colors are just Matplotlib defaults...\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "number_of_train_images = X_train.shape[0]\n",
    "number_of_train_images_to_show = 2 # for example, but feel free to show more images\n",
    "random_list_of_images_to_show = np.random.randint(0, number_of_train_images , number_of_train_images_to_show)\n",
    "\n",
    "for i in random_list_of_images_to_show:\n",
    "    plt.imshow(X_train[i], cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.2) Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Neural Networks converge faster when the input data is somehow normalized** ‚ùóÔ∏è\n",
    "\n",
    "üë©üèª‚Äçüè´ How do we proceed for Convolutional Neural Networks ?\n",
    "* The `RBG` intensities are coded between 0 and 255. \n",
    "* We can simply divide the input data by the maximal value 255 to have all the pixels' intensities between 0 and 1 üòâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: As a first preprocessing step, please normalize your data.** ‚ùì\n",
    "\n",
    "Don't forget to do it both on your train data and your test data.\n",
    "\n",
    "(*Note: you can also center your data, by subtracting 0.5 from all the values, but it is not mandatory*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "X_train = X_train / 255.\n",
    "X_test = X_test / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.3) Inputs' dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: What is the shape of your images** ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÜ You see that you have 60,000 training images, all of size $(28, 28)$. However...\n",
    "\n",
    "‚ùóÔ∏è  **`Convolutional Neural Network models need to be fed with images whose last dimension is the number of channels`** ‚ùóÔ∏è\n",
    "\n",
    "This last dimension is clearly missing here... Can you guess the reason why?\n",
    "\n",
    "<details>\n",
    "    <summary><i>Answer<i></summary>\n",
    "        \n",
    "* All these $60000$ $ 28 \\times 28 $ pictures are black-and-white. $ \\implies $ Each pixel lives on a spectrum from full black (0) to full white (1)\n",
    "        \n",
    "* Theoretically, you don't need to know the number of channels unlike colored pictures using for example:\n",
    "    - the RGB system with 3 channels (<b><span style=\"color:red\">Red</span> <span style=\"color:green\">Green</span> <span style=\"color:blue\">Blue</span></b>)\n",
    "    - the CYMK system  with 4 channels (<b><span style=\"color:cyan\">Cyan</span> <span style=\"color:magenta\">Magenta</span> <span style=\"color:yellow\">Yellow</span> <span style=\"color:black\">Black</span> </b>)\n",
    "        \n",
    "        \n",
    "</details>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: expanding dimensions** ‚ùì\n",
    "\n",
    "* Use the **expand_dims** to add one dimension at the end of the training data and test data.\n",
    "\n",
    "* Then, print the shapes of `X_train` and `X_test` which should respectively be equal to $(60000, 28, 28, 1)$ and $(10000, 28, 28, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.backend import expand_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-21 16:27:18.651571: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# $CHALLENGIFY_BEGIN\n",
    "X_train = expand_dims(X_train, axis=-1)\n",
    "X_test = expand_dims(X_test, axis=-1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.4) Label encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing to do to prepare your data is to convert your labels to \"*one-hot encode*\" the categories.\n",
    "\n",
    "‚ùì **Question: encoding the labels** ‚ùì \n",
    "\n",
    "* Use **`to_categorical`** to transform your labels. \n",
    "* Store the results into two variables that you can call **`y_train_cat`** and **`y_test_cat`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# $CHALLENGIFY_BEGIN\n",
    "y_train_cat = to_categorical(y_train, num_classes=10)\n",
    "y_test_cat = to_categorical(y_test, num_classes=10)\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "delete"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train_cat.shape = (60000, 10)\n",
      "y_test_cat.shape = (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(f\"y_train_cat.shape = {y_train_cat.shape}\")\n",
    "print(f\"y_test_cat.shape = {y_test_cat.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now ready to be used. ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) The Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Architecture and compilation of a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "‚ùì **Question: CNN Architecture and compilation** ‚ùì\n",
    "\n",
    "Now, let's build a <u>Convolutional Neural Network</u> that has: \n",
    "\n",
    "\n",
    "- a `Conv2D` layer with 8 filters, each of size $(4, 4)$, an input shape suitable for your task, the `relu` activation function, and `padding='same'`\n",
    "- a `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n",
    "- a second `Conv2D` layer with 16 filters, each of size $(3, 3)$, and the `relu` activation function\n",
    "- a second `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n",
    "\n",
    "\n",
    "- a `Flatten` layer\n",
    "- a first `Dense` layer with 10 neurons and the `relu` activation function\n",
    "- a last (predictive) layer that is suited for your task\n",
    "\n",
    "In the function that initializes this model, do not forget to include the <u>compilation of the model</u>, which:\n",
    "* optimizes the `categorical_crossentropy` \n",
    "* with the `adam` optimizer\n",
    "* and the `accuracy` as the metrics\n",
    "\n",
    "(*Note: you could add more classification metrics if you want but the dataset is well balanced!*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "\n",
    "def initialize_model():\n",
    "\n",
    "    model = models.Sequential()\n",
    "\n",
    "    ### First Convolution & MaxPooling\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    ### Second Convolution & MaxPooling\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    ### Flattening\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    ### One Fully Connected layer - \"Fully Connected\" is equivalent to saying \"Dense\"\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    ### Last layer - Classification Layer with 10 outputs corresponding to 10 digits\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    ### Model compilation\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "delete"
    ]
   },
   "outputs": [],
   "source": [
    "##############\n",
    "### Answer ###\n",
    "##############\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "def initialize_model():\n",
    "    model = models.Sequential()\n",
    "\n",
    "    ### First Convolution & MaxPooling\n",
    "    model.add(layers.Conv2D(8, (4,4), input_shape=(28, 28, 1), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "    \n",
    "    ### Second Convolution & MaxPooling\n",
    "    model.add(layers.Conv2D(16, (3,3), activation='relu', padding='same'))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "    ### Flattening\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "    ### One Fully Connected layer - \"Fully Connected\" is equivalent to saying \"Dense\"\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "\n",
    "    ### Last layer - Classification Layer with 10 outputs corresponding to 10 digits\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    ### Model compilation\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = initialize_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: number of trainable parameters in a convolutional layer** ‚ùì \n",
    "\n",
    "How many trainable parameters are there in your model?\n",
    "1. Compute them with ***model.summary( )*** first\n",
    "2. Recompute them manually to make sure you properly understood ***what influences the number of weights in a CNN***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 28, 28, 8)         136       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 8)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 16)        1168      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 7, 7, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 784)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                7850      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,264\n",
      "Trainable params: 9,264\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "delete"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First Conv2D\n",
    "first_layer_weights = 8 * (4*4) * 1 + 8\n",
    "first_layer_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "delete"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1168"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second Conv2D\n",
    "second_layer_weights = 16 * (3*3) * 8 + 16\n",
    "second_layer_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "delete"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7850"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Third Conv2D\n",
    "third_layer_weights = 10 * 7 * 7 * 16 + 10\n",
    "third_layer_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "delete"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dense Layer\n",
    "dense_layer_weights = 10 * 10 + 10\n",
    "dense_layer_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "delete"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9264"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_number_of_weights = first_layer_weights + second_layer_weights + third_layer_weights + dense_layer_weights\n",
    "total_number_of_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) Training a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: training a CNN** ‚ùì \n",
    "\n",
    "Initialize your model and fit it on the train data. \n",
    "- Do not forget to use a **Validation Set/Split** and an **Early Stopping criterion**. \n",
    "- Limit yourself to 5 epochs max in this challenge, just to save some precious time for the more advanced challenges!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1313/1313 [==============================] - 12s 9ms/step - loss: 0.4121 - accuracy: 0.8672 - val_loss: 0.1418 - val_accuracy: 0.9565\n",
      "Epoch 2/5\n",
      "1313/1313 [==============================] - 11s 8ms/step - loss: 0.1244 - accuracy: 0.9619 - val_loss: 0.1230 - val_accuracy: 0.9623\n",
      "Epoch 3/5\n",
      "1313/1313 [==============================] - 11s 8ms/step - loss: 0.0929 - accuracy: 0.9721 - val_loss: 0.1039 - val_accuracy: 0.9659\n",
      "Epoch 4/5\n",
      "1313/1313 [==============================] - 11s 8ms/step - loss: 0.0764 - accuracy: 0.9766 - val_loss: 0.0849 - val_accuracy: 0.9741\n",
      "Epoch 5/5\n",
      "1313/1313 [==============================] - 10s 8ms/step - loss: 0.0666 - accuracy: 0.9799 - val_loss: 0.0754 - val_accuracy: 0.9783\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# $CHALLENGIFY_BEGIN\n",
    "\n",
    "model = initialize_model()\n",
    "\n",
    "es = EarlyStopping(patience = 5)\n",
    "\n",
    "history = model.fit(X_train, \n",
    "                    y_train_cat,\n",
    "                    validation_split = 0.3,\n",
    "                    batch_size = 32,\n",
    "                    epochs = 5,\n",
    "                    callbacks = [es],\n",
    "                    verbose = 1)\n",
    "\n",
    "# $CHALLENGIFY_END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: How many iterations does the CNN perform per epoch** ‚ùì\n",
    "\n",
    "_Note: it has nothing to do with the fact that this is a CNN. This is related to the concept of forward/backward propagation already covered during the previous lecture on optimizers, fitting, and losses üòâ_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><i>Answer:</i></summary>\n",
    "\n",
    "With `verbose = 1` when fitting your model, you have access to crucial information about your training procedure.\n",
    "    \n",
    "Remember that we've just trained our CNN model on $60000$ training images\n",
    "\n",
    "If the chosen batch size is 32: \n",
    "\n",
    "* For each epoch, we have $ \\large \\lceil \\frac{60000}{32} \\rceil = 1875$ minibatches <br/>\n",
    "* The _validation_split_ is equal to $0.3$ - which means that within one single epoch, there are:\n",
    "    * $ \\lceil 1875 \\times (1 - 0.3) \\rceil = \\lceil 1312.5 \\rceil = 1313$ batches are used to compute the `train_loss` \n",
    "    * $ 1875 - 1312 = 562 $ batches are used to compute the `val_loss`\n",
    "    * **The parameters are updated 1313 times per epoch** as there are 1313 forward/backward propagations per epoch !!!\n",
    "\n",
    "\n",
    "üëâ With so many updates of the weights within one epoch, you can understand why this CNN model converges even with a limited number of epochs.\n",
    "\n",
    "</details>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.3) Evaluating its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Evaluating your CNN** ‚ùì \n",
    "\n",
    "What is your **`accuracy on the test set?`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0606 - accuracy: 0.9816\n",
      "The accuracy on the test set is of 98.16 %\n"
     ]
    }
   ],
   "source": [
    "res = model.evaluate(X_test, y_test_cat, verbose = 1 )\n",
    "print(f'The accuracy on the test set is of {res[1]*100:.2f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ You should already be impressed by your CNN skills! Reaching over 95% accuracy!\n",
    "\n",
    "üî• You solved what was a very hard problem 30 years ago with your own CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ **Congratulations!**\n",
    "\n",
    "üíæ Don't forget to `git add/commit/push` your notebook...\n",
    "\n",
    "üöÄ ... and move on to the next challenge!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
